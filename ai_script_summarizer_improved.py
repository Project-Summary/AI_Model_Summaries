# ai_script_summarizer_improved.py
import json
import pickle
import re
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
import nltk
from collections import defaultdict, Counter
import pymongo
from pymongo import MongoClient
from bson import ObjectId
import threading
import time
import warnings
warnings.filterwarnings('ignore')

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize

class ImprovedScriptSummarizerAI:
    def __init__(self, mongodb_uri="mongodb+srv://nguyenvanninh:MM1onTUoA1sF4AnZ@onebizai.tt3rs.mongodb.net/", db_name="test"):
        """
        AI t√≥m t·∫Øt k·ªãch b·∫£n c·∫£i thi·ªán v·ªõi MongoDB
        """
        self.client = MongoClient(mongodb_uri)
        self.db = self.client[db_name]
        
        # Collections theo schema m·ªõi
        self.scripts_collection = self.db.scripts  # Collection Script
        self.feedbacks_collection = self.db.feedbacks  # Collection Feedback  
        self.categories_collection = self.db.categories  # Collection Category
        
        # Collections cho AI training
        self.training_metrics_collection = self.db.training_metrics
        self.learned_patterns_collection = self.db.learned_patterns
        self.ai_summaries_collection = self.db.ai_summaries  # L∆∞u k·∫øt qu·∫£ AI t√≥m t·∫Øt
        
        # AI Models
        self.vectorizer = TfidfVectorizer(
            max_features=2000,
            stop_words=self._get_vietnamese_stopwords(),
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.8
        )
        
        self.quality_predictor = GradientBoostingRegressor(
            n_estimators=150, 
            learning_rate=0.1,
            max_depth=6,
            random_state=42
        )
        
        self.genre_classifier = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            random_state=42
        )
        
        # Episode templates v·ªõi ƒë·ªô d√†i A4 (kho·∫£ng 500-600 t·ª´)
        self.episode_templates = {
            'drama': {
                'intro': {'ratio': 0.15, 'target_words': 150},
                'development': {'ratio': 0.35, 'target_words': 200},
                'climax': {'ratio': 0.30, 'target_words': 180},
                'resolution': {'ratio': 0.20, 'target_words': 120}
            },
            'comedy': {
                'setup': {'ratio': 0.25, 'target_words': 150},
                'conflict': {'ratio': 0.50, 'target_words': 250},
                'resolution': {'ratio': 0.25, 'target_words': 150}
            },
            'action': {
                'setup': {'ratio': 0.20, 'target_words': 120},
                'action_sequence': {'ratio': 0.60, 'target_words': 300},
                'aftermath': {'ratio': 0.20, 'target_words': 130}
            },
            'romance': {
                'meeting': {'ratio': 0.30, 'target_words': 180},
                'development': {'ratio': 0.40, 'target_words': 220},
                'resolution': {'ratio': 0.30, 'target_words': 150}
            }
        }
        
        self.knowledge_base = []
        self._load_models()
        self._create_indexes()
        
    def _create_indexes(self):
        """T·∫°o indexes cho MongoDB ƒë·ªÉ t·ªëi ∆∞u performance"""
        try:
            # Indexes cho AI collections
            self.ai_summaries_collection.create_index([("created_at", -1)])
            self.ai_summaries_collection.create_index([("script_id", 1)])
            self.ai_summaries_collection.create_index([("genre", 1)])
            self.ai_summaries_collection.create_index([("quality_score", -1)])
            
            self.training_metrics_collection.create_index([("trained_at", -1)])
            self.learned_patterns_collection.create_index([("learned_at", -1)])
            
            print("‚úÖ MongoDB indexes created successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è Error creating indexes: {e}")
    
    def _get_vietnamese_stopwords(self):
        """T·∫°o danh s√°ch stopwords ti·∫øng Vi·ªát m·ªü r·ªông"""
        vietnamese_stopwords = [
            'l√†', 'c·ªßa', 'v√†', 'c√≥', 'cho', 'v·ªõi', 'trong', 'kh√¥ng', 'ƒë∆∞·ª£c', 'm·ªôt',
            'n√†y', 'ƒë√≥', 'nh·ªØng', 'c√°c', 'ƒë·ªÉ', 't·ª´', 'tr√™n', 'v·ªÅ', 'khi', 'n·∫øu',
            'm√†', 'nh∆∞', 'ƒë√£', 's·∫Ω', 'b·ªã', 'hay', 'th√¨', 'ho·∫∑c', 'nh∆∞ng', 'v√¨',
            't·∫°i', 'theo', 'gi·ªØa', 'sau', 'tr∆∞·ªõc', 'ngo√†i', 'd∆∞·ªõi', 'l√™n', 'xu·ªëng',
            'c≈©ng', 'ƒë·ªÅu', 'ph·∫£i', 'ch·ªâ', 'l·∫°i', 'c√≤n', 'h∆°n', 'n·ªØa', 'kh√°c',
            'nhi·ªÅu', '√≠t', 'l·ªõn', 'nh·ªè', 'cao', 'th·∫•p', 'xa', 'g·∫ßn', 'm·ªõi', 'c≈©'
        ]
        
        try:
            english_stopwords = list(stopwords.words('english'))
            return vietnamese_stopwords + english_stopwords
        except:
            return vietnamese_stopwords
    
    def get_categories_from_db(self) -> List[Dict]:
        """L·∫•y danh s√°ch categories t·ª´ DB"""
        try:
            categories = list(self.categories_collection.find(
                {"isActive": True},
                {"name": 1, "slug": 1, "description": 1, "type": 1}
            ))
            return categories
        except Exception as e:
            print(f"‚ùå Error fetching categories: {e}")
            return []
    
    def detect_genre_from_categories(self, text: str) -> Tuple[str, float, Optional[ObjectId]]:
        """Ph√°t hi·ªán th·ªÉ lo·∫°i d·ª±a tr√™n categories trong DB"""
        text_lower = text.lower()
        categories = self.get_categories_from_db()
        
        if not categories:
            # Fallback to default detection
            genre, confidence = self.detect_genre(text)
            return genre, confidence, None
        
        category_scores = {}
        
        for category in categories:
            category_name = category['name'].lower()
            category_slug = category['slug'].lower()
            category_desc = category.get('description', '').lower()
            
            score = 0
            
            # T√¨m ki·∫øm trong text
            if category_name in text_lower:
                score += 3
            if category_slug in text_lower:
                score += 2
            
            # T√¨m ki·∫øm keywords li√™n quan
            if 'h√†nh ƒë·ªông' in category_name or 'action' in category_name:
                action_keywords = ['ƒë√°nh nhau', 'chi·∫øn ƒë·∫•u', 's√∫ng', 'n·ªï', 'r∆∞·ª£t ƒëu·ªïi', 'h√†nh ƒë·ªông']
                score += sum(2 for keyword in action_keywords if keyword in text_lower)
            
            elif 't√¨nh c·∫£m' in category_name or 'romance' in category_name:
                romance_keywords = ['y√™u', 't√¨nh y√™u', 'h·∫πn h√≤', 'c∆∞·ªõi', 'l√£ng m·∫°n']
                score += sum(2 for keyword in romance_keywords if keyword in text_lower)
            
            elif 'h√†i' in category_name or 'comedy' in category_name:
                comedy_keywords = ['h√†i h∆∞·ªõc', 'vui nh·ªôn', 'c∆∞·ªùi', 'h√†i', 'bu·ªìn c∆∞·ªùi']
                score += sum(2 for keyword in comedy_keywords if keyword in text_lower)
            
            elif 'kinh d·ªã' in category_name or 'thriller' in category_name:
                thriller_keywords = ['b√≠ ·∫©n', 'gi·∫øt', 'ch·∫øt', 'nguy hi·ªÉm', 's·ª£ h√£i', 'kinh d·ªã']
                score += sum(2 for keyword in thriller_keywords if keyword in text_lower)
            
            if score > 0:
                category_scores[category['_id']] = {
                    'score': score,
                    'name': category['name'],
                    'slug': category['slug']
                }
        
        if not category_scores:
            # Fallback to default detection
            genre, confidence = self.detect_genre(text)
            return genre, confidence, None
        
        # T√¨m category c√≥ ƒëi·ªÉm cao nh·∫•t
        best_category_id = max(category_scores, key=lambda x: category_scores[x]['score'])
        best_category = category_scores[best_category_id]
        
        total_score = sum(cat['score'] for cat in category_scores.values())
        confidence = best_category['score'] / total_score if total_score > 0 else 0.5
        
        return best_category['name'], min(confidence, 1.0), best_category_id
    
    def detect_genre(self, text: str) -> Tuple[str, float]:
        """Ph√°t hi·ªán th·ªÉ lo·∫°i v·ªõi confidence score (fallback method)"""
        text_lower = text.lower()
        
        genre_keywords = {
            'action': {
                'keywords': ['ƒë√°nh nhau', 'chi·∫øn ƒë·∫•u', 's√∫ng', 'n·ªï', 'r∆∞·ª£t ƒëu·ªïi', 'h√†nh ƒë·ªông', 'phi√™u l∆∞u', 't·∫•n c√¥ng', 'b·∫°o l·ª±c'],
                'weight': [3, 3, 2, 2, 2, 1, 1, 2, 2]
            },
            'drama': {
                'keywords': ['c·∫£m x√∫c', 't√¨nh c·∫£m', 'gia ƒë√¨nh', 'n∆∞·ªõc m·∫Øt', 'ƒëau kh·ªï', 'bi k·ªãch', 'x√∫c ƒë·ªông', 't√¢m l√Ω'],
                'weight': [2, 2, 3, 2, 2, 3, 2, 2]
            },
            'comedy': {
                'keywords': ['h√†i h∆∞·ªõc', 'vui nh·ªôn', 'c∆∞·ªùi', 'h√†i', 'bu·ªìn c∆∞·ªùi', 'vui v·∫ª', 'h√≥m h·ªânh', 'd√≠ d·ªèm'],
                'weight': [3, 2, 3, 3, 2, 1, 2, 2]
            },
            'romance': {
                'keywords': ['y√™u', 't√¨nh y√™u', 'h·∫πn h√≤', 'c∆∞·ªõi', 'l√£ng m·∫°n', 't√¨nh c·∫£m', 'ƒë√°m c∆∞·ªõi', 'valentine'],
                'weight': [3, 3, 2, 2, 3, 2, 2, 1]
            },
            'thriller': {
                'keywords': ['b√≠ ·∫©n', 'gi·∫øt', 'ch·∫øt', 'nguy hi·ªÉm', 's·ª£ h√£i', 'kinh d·ªã', 'cƒÉng th·∫≥ng', 'h·ªìi h·ªôp'],
                'weight': [2, 3, 2, 2, 2, 3, 2, 2]
            },
            'fantasy': {
                'keywords': ['ph√©p thu·∫≠t', 'ma thu·∫≠t', 'th·∫ßn ti√™n', 'r·ªìng', 'ph√π th·ªßy', 'si√™u nhi√™n', 'k·ª≥ ·∫£o'],
                'weight': [3, 3, 2, 2, 2, 2, 2]
            }
        }
        
        genre_scores = {}
        for genre, data in genre_keywords.items():
            score = 0
            for keyword, weight in zip(data['keywords'], data['weight']):
                if keyword in text_lower:
                    score += weight
            genre_scores[genre] = score
        
        if not genre_scores or max(genre_scores.values()) == 0:
            return 'drama', 0.5
        
        best_genre = max(genre_scores, key=genre_scores.get)
        total_score = sum(genre_scores.values())
        confidence = genre_scores[best_genre] / total_score if total_score > 0 else 0.5
        
        return best_genre, min(confidence, 1.0)
    
    def extract_key_scenes(self, text: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t c·∫£nh quan tr·ªçng v·ªõi thu·∫≠t to√°n c·∫£i thi·ªán"""
        sentences = sent_tokenize(text)
        
        if len(sentences) == 0:
            return []
        
        # T√≠nh TF-IDF scores
        try:
            tfidf_matrix = self.vectorizer.fit_transform(sentences)
            sentence_scores = np.mean(tfidf_matrix.toarray(), axis=1)
        except:
            sentence_scores = np.ones(len(sentences))
        
        # Keywords quan tr·ªçng v·ªõi tr·ªçng s·ªë
        important_keywords = {
            'plot': ['b·∫•t ng·ªù', 'shock', 'ƒë·ªôt nhi√™n', 'b·ªóng nhi√™n', 't√¨nh ti·∫øt'],
            'structure': ['b·∫Øt ƒë·∫ßu', 'm·ªü ƒë·∫ßu', 'cu·ªëi c√πng', 'k·∫øt th√∫c', 'ch·∫•m d·ª©t'],
            'conflict': ['xung ƒë·ªôt', 'tranh c√£i', 'ƒë·ªëi ƒë·∫ßu', 'ch·ªëng l·∫°i', 'c√£i nhau'],
            'emotion': ['c·∫£m x√∫c', 'x√∫c ƒë·ªông', 't·ª©c gi·∫≠n', 'vui m·ª´ng', 'bu·ªìn b√£']
        }
        
        scenes = []
        for i, sentence in enumerate(sentences):
            importance = sentence_scores[i]
            
            # TƒÉng ƒëi·ªÉm d·ª±a tr√™n keywords
            for category, keywords in important_keywords.items():
                for keyword in keywords:
                    if keyword in sentence.lower():
                        if category == 'plot':
                            importance += 0.8
                        elif category == 'structure':
                            importance += 0.6
                        elif category == 'conflict':
                            importance += 0.7
                        elif category == 'emotion':
                            importance += 0.5
            
            # TƒÉng ƒëi·ªÉm cho c√¢u c√≥ ƒë·ªô d√†i v·ª´a ph·∫£i
            sentence_length = len(sentence.split())
            if 10 <= sentence_length <= 30:
                importance += 0.3
            
            scenes.append({
                'text': sentence,
                'position': i / len(sentences),
                'importance': importance,
                'scene_type': self._classify_scene_type(sentence),
                'word_count': sentence_length
            })
        
        # S·∫Øp x·∫øp theo ƒë·ªô quan tr·ªçng
        scenes.sort(key=lambda x: x['importance'], reverse=True)
        return scenes
    
    def _classify_scene_type(self, text: str) -> str:
        """Ph√¢n lo·∫°i lo·∫°i c·∫£nh chi ti·∫øt h∆°n"""
        text_lower = text.lower()
        
        scene_patterns = {
            'dialogue': ['n√≥i', 'h·ªèi', 'tr·∫£ l·ªùi', 'n√≥i chuy·ªán', 'th√¨ th·∫ßm', 'h√©t l√™n', 'th·ªët l√™n'],
            'action': ['ƒë√°nh', 'ch·∫°y', 'nh·∫£y', 'ƒëu·ªïi', 't·∫•n c√¥ng', 'b·ªè ch·∫°y', 'lao t·ªõi'],
            'internal': ['nghƒ©', 'c·∫£m th·∫•y', 't√¢m tr·∫°ng', 'suy nghƒ©', 'nh·ªõ l·∫°i', 'h·ªëi h·∫≠n'],
            'description': ['c·∫£nh', 'khung c·∫£nh', 'b·ªëi c·∫£nh', 'm√¥i tr∆∞·ªùng', 'kh√¥ng gian'],
            'narrative': []  # Default
        }
        
        for scene_type, keywords in scene_patterns.items():
            if any(keyword in text_lower for keyword in keywords):
                return scene_type
        
        return 'narrative'
    
    def create_a4_episode_summary(self, scenes: List[Dict], episode_num: int, genre: str) -> Dict:
        """T·∫°o t√≥m t·∫Øt t·∫≠p phim ƒë·ªô d√†i 1 trang A4 (550-650 t·ª´)"""
        if not scenes:
            return {
                'episode_number': episode_num,
                'title': f"T·∫≠p {episode_num}",
                'summary': f"T·∫≠p {episode_num}: N·ªôi dung ƒëang ƒë∆∞·ª£c c·∫≠p nh·∫≠t.",
                'word_count': 0,
                'sections': {}
            }
        
        template = self.episode_templates.get(genre, self.episode_templates['drama'])
        
        # Ph√¢n chia c·∫£nh theo v·ªã tr√≠
        intro_scenes = [s for s in scenes if s['position'] < 0.3]
        middle_scenes = [s for s in scenes if 0.3 <= s['position'] < 0.7]
        end_scenes = [s for s in scenes if s['position'] >= 0.7]
        
        sections = {}
        total_words = 0
        
        # T·∫°o t·ª´ng ph·∫ßn theo template
        for section_name, section_config in template.items():
            target_words = section_config['target_words']
            
            if section_name in ['intro', 'setup', 'meeting']:
                section_scenes = intro_scenes[:3]
            elif section_name in ['development', 'conflict', 'action_sequence']:
                section_scenes = middle_scenes[:4]
            else:  # resolution, aftermath
                section_scenes = end_scenes[:2]
            
            section_summary = self._create_detailed_section_summary(
                section_scenes, section_name, target_words
            )
            
            sections[section_name] = {
                'title': section_name.title(),
                'content': section_summary,
                'word_count': len(section_summary.split())
            }
            
            total_words += len(section_summary.split())
        
        # T·∫°o t√≥m t·∫Øt ho√†n ch·ªânh
        full_summary = self._combine_sections_to_a4(sections, episode_num)
        
        return {
            'episode_number': episode_num,
            'title': f"T·∫≠p {episode_num}",
            'summary': full_summary,
            'word_count': len(full_summary.split()),
            'sections': sections,
            'genre': genre,
            'target_length': '1 trang A4 (550-650 t·ª´)'
        }
    
    def _create_detailed_section_summary(self, scenes: List[Dict], section_name: str, target_words: int) -> str:
        """T·∫°o t√≥m t·∫Øt chi ti·∫øt cho m·ªôt ph·∫ßn"""
        if not scenes:
            return f"Ph·∫ßn {section_name}: N·ªôi dung ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn."
        
        # Tr√≠ch xu·∫•t th√¥ng tin ch√≠nh
        main_events = []
        characters = set()
        locations = set()
        emotions = []
        
        for scene in scenes:
            text = scene['text']
            
            # Tr√≠ch xu·∫•t nh√¢n v·∫≠t (t·ª´ vi·∫øt hoa)
            potential_chars = re.findall(r'\b[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+', text)
            characters.update([char for char in potential_chars if len(char) > 2 and char not in ['T√¥i', 'Anh', 'Ch·ªã', 'Em']])
            
            # Tr√≠ch xu·∫•t ƒë·ªãa ƒëi·ªÉm
            location_keywords = ['t·∫°i', '·ªü', 'trong', 'ngo√†i', 'tr√™n', 'd∆∞·ªõi']
            for keyword in location_keywords:
                if keyword in text.lower():
                    words_after = text.lower().split(keyword)[1:2]
                    if words_after:
                        potential_location = words_after[0].split()[:3]
                        locations.add(' '.join(potential_location).strip(' .,!?'))
            
            # Tr√≠ch xu·∫•t c·∫£m x√∫c
            emotion_keywords = ['vui', 'bu·ªìn', 't·ª©c gi·∫≠n', 'h·∫°nh ph√∫c', 'lo l·∫Øng', 's·ª£ h√£i', 'b·∫•t ng·ªù']
            for emotion in emotion_keywords:
                if emotion in text.lower():
                    emotions.append(emotion)
            
            # Tr√≠ch xu·∫•t s·ª± ki·ªán ch√≠nh (c√¢u ng·∫Øn g·ªçn)
            if len(text.split()) <= 25:  # C√¢u ng·∫Øn, c√≥ th·ªÉ l√† event ch√≠nh
                main_events.append(text[:100] + '...' if len(text) > 100 else text)
        
        # T·∫°o t√≥m t·∫Øt d·ª±a tr√™n th√¥ng tin ƒë√£ tr√≠ch xu·∫•t
        summary_parts = []
        
        # Nh√¢n v·∫≠t
        if characters:
            char_list = list(characters)[:3]  # L·∫•y t·ªëi ƒëa 3 nh√¢n v·∫≠t
            summary_parts.append(f"Nh√¢n v·∫≠t ch√≠nh: {', '.join(char_list)}")
        
        # S·ª± ki·ªán
        if main_events:
            event_summary = '. '.join(main_events[:2])  # L·∫•y 2 s·ª± ki·ªán ch√≠nh
            summary_parts.append(f"Di·ªÖn bi·∫øn: {event_summary}")
        
        # C·∫£m x√∫c
        if emotions:
            unique_emotions = list(set(emotions))[:2]
            summary_parts.append(f"T√¢m tr·∫°ng: {', '.join(unique_emotions)}")
        
        # ƒê·ªãa ƒëi·ªÉm
        if locations:
            location_list = [loc for loc in locations if len(loc.strip()) > 3][:2]
            if location_list:
                summary_parts.append(f"B·ªëi c·∫£nh: {', '.join(location_list)}")
        
        # K·∫øt h·ª£p v√† ƒëi·ªÅu ch·ªânh ƒë·ªô d√†i
        base_summary = '. '.join(summary_parts) + '.'
        
        # M·ªü r·ªông n·∫øu c·∫ßn ƒë·ªÉ ƒë·∫°t target_words
        current_words = len(base_summary.split())
        if current_words < target_words * 0.8:  # N·∫øu qu√° ng·∫Øn
            # Th√™m chi ti·∫øt t·ª´ scenes
            additional_details = []
            for scene in scenes[:2]:
                if scene['scene_type'] == 'dialogue':
                    additional_details.append(f"ƒê·ªëi tho·∫°i: {scene['text'][:80]}...")
                elif scene['scene_type'] == 'action':
                    additional_details.append(f"H√†nh ƒë·ªông: {scene['text'][:80]}...")
            
            if additional_details:
                base_summary += ' ' + ' '.join(additional_details)
        
        return base_summary
    
    def _combine_sections_to_a4(self, sections: Dict, episode_num: int) -> str:
        """K·∫øt h·ª£p c√°c ph·∫ßn th√†nh t√≥m t·∫Øt A4 ho√†n ch·ªânh"""
        header = f"=== T·∫¨P {episode_num} ===\n\n"
        
        content_parts = []
        for section_name, section_data in sections.items():
            section_title = section_data['title']
            section_content = section_data['content']
            
            formatted_section = f"„Äê{section_title}„Äë\n{section_content}\n"
            content_parts.append(formatted_section)
        
        # Th√™m th·ªëng k√™ cu·ªëi
        total_words = sum(section['word_count'] for section in sections.values())
        footer = f"\n--- T·ªïng s·ªë t·ª´: {total_words} ---"
        
        full_content = header + '\n'.join(content_parts) + footer
        
        # ƒê·∫£m b·∫£o ƒë·ªô d√†i ph√π h·ª£p (550-650 t·ª´)
        current_words = len(full_content.split())
        if current_words > 650:
            # C·∫Øt b·ªõt n·∫øu qu√° d√†i
            words = full_content.split()[:650]
            full_content = ' '.join(words) + '...'
        elif current_words < 550:
            # Th√™m n·ªôi dung n·∫øu qu√° ng·∫Øn
            padding = "\n\nGhi ch√∫: T·∫≠p n√†y ch·ª©a nhi·ªÅu t√¨nh ti·∫øt quan tr·ªçng v√† c·∫£m x√∫c s√¢u s·∫Øc, ƒë√≥ng vai tr√≤ then ch·ªët trong c·ªët truy·ªán t·ªïng th·ªÉ."
            full_content += padding
        
        return full_content
    
    def summarize_script(self, script: str, episode_number: int = 3, script_id: Optional[str] = None) -> Dict:
        """T√≥m t·∫Øt k·ªãch b·∫£n ch√≠nh v·ªõi format A4"""
        # Ph√°t hi·ªán th·ªÉ lo·∫°i t·ª´ categories trong DB
        genre, genre_confidence, category_id = self.detect_genre_from_categories(script)
        
        # Tr√≠ch xu·∫•t c·∫£nh quan tr·ªçng
        all_scenes = self.extract_key_scenes(script)
        
        if not all_scenes:
            return {
                'error': 'Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ k·ªãch b·∫£n',
                'script_length': len(script),
                'word_count': len(script.split())
            }
        
        # Chia scenes th√†nh c√°c t·∫≠p
        scenes_per_episode = len(all_scenes) // episode_number
        episodes = []
        
        for ep_num in range(episode_number):
            start_idx = ep_num * scenes_per_episode
            end_idx = start_idx + scenes_per_episode if ep_num < episode_number - 1 else len(all_scenes)
            
            episode_scenes = all_scenes[start_idx:end_idx]
            episode_summary = self.create_a4_episode_summary(episode_scenes, ep_num + 1, genre)
            episodes.append(episode_summary)
        
        # T·∫°o t√≥m t·∫Øt t·ªïng quan
        overall_summary = self._create_overall_summary(script, genre, all_scenes)
        
        result = {
            'overall_summary': overall_summary,
            'genre': genre,
            'genre_confidence': genre_confidence,
            'category_id': str(category_id) if category_id else None,
            'total_episodes': episode_number,
            'episodes': episodes,
            'created_at': datetime.now(),
            'script_stats': {
                'original_length': len(script),
                'word_count': len(script.split()),
                'sentence_count': len(sent_tokenize(script)),
                'total_scenes_extracted': len(all_scenes)
            }
        }
        
        # L∆∞u v√†o MongoDB
        self._save_to_database(script, result, script_id)
        
        return result
    
    def _create_overall_summary(self, text: str, genre: str, scenes: List[Dict]) -> str:
        """T·∫°o t√≥m t·∫Øt t·ªïng quan chi ti·∫øt"""
        # Ph√¢n t√≠ch nh√¢n v·∫≠t ch√≠nh
        characters = self._extract_main_characters(text)
        
        # Ph√¢n t√≠ch xung ƒë·ªôt ch√≠nh
        main_conflicts = self._extract_main_conflicts(scenes)
        
        # Ph√¢n t√≠ch theme ch√≠nh
        themes = self._extract_themes(text, genre)
        
        summary_parts = [
            f"üé≠ Th·ªÉ lo·∫°i: {genre.title()}",
            f"üë• Nh√¢n v·∫≠t ch√≠nh: {', '.join(characters[:3]) if characters else 'ƒêang ph√¢n t√≠ch'}",
            f"‚öîÔ∏è Xung ƒë·ªôt ch√≠nh: {main_conflicts[0] if main_conflicts else 'Xung ƒë·ªôt n·ªôi t√¢m v√† ho√†n c·∫£nh'}",
            f"üéØ Ch·ªß ƒë·ªÅ: {', '.join(themes[:2]) if themes else 'T√¨nh y√™u v√† cu·ªôc s·ªëng'}",
            f"üìä C·∫•u tr√∫c: Chia th√†nh {len(scenes)} c·∫£nh ch√≠nh v·ªõi nhi·ªÅu t√¨nh ti·∫øt h·∫•p d·∫´n"
        ]
        
        return '\n'.join(summary_parts)
    
    def _extract_main_characters(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t nh√¢n v·∫≠t ch√≠nh th√¥ng minh h∆°n"""
        # T√¨m t√™n ri√™ng (t·ª´ vi·∫øt hoa)
        potential_names = re.findall(r'\b[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+', text)
        
        # L·ªçc b·ªè t·ª´ th√¥ng th∆∞·ªùng
        common_words = ['T√¥i', 'Anh', 'Ch·ªã', 'Em', '√îng', 'B√†', 'C√¥', 'Ch√∫', 'Th·∫ßy', 'C√¥', 'M·ªôt', 'Hai', 'Ba']
        
        # ƒê·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán
        name_counts = Counter([name for name in potential_names 
                              if name not in common_words and len(name) > 2])
        
        # L·∫•y nh·ªØng t√™n xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
        main_characters = [name for name, count in name_counts.most_common(5) if count >= 2]
        
        return main_characters
    
    def _extract_main_conflicts(self, scenes: List[Dict]) -> List[str]:
        """Tr√≠ch xu·∫•t xung ƒë·ªôt ch√≠nh"""
        conflict_keywords = [
            'xung ƒë·ªôt', 'tranh c√£i', 'ƒë·ªëi ƒë·∫ßu', 'ch·ªëng l·∫°i', 'b·∫•t ƒë·ªìng',
            'c√£i nhau', 'ph·∫£n ƒë·ªëi', 'kh√¥ng ƒë·ªìng √Ω', 'b·∫•t h√≤a', 'm√¢u thu·∫´n'
        ]
        
        conflicts = []
        for scene in scenes:
            text = scene['text'].lower()
            for keyword in conflict_keywords:
                if keyword in text:
                    # L·∫•y c√¢u ch·ª©a t·ª´ kh√≥a xung ƒë·ªôt
                    sentences = scene['text'].split('.')
                    for sentence in sentences:
                        if keyword in sentence.lower():
                            conflicts.append(sentence.strip()[:100] + '...')
                            break
                    break
        
        return conflicts[:3]
    
    def _extract_themes(self, text: str, genre: str) -> List[str]:
        """Tr√≠ch xu·∫•t ch·ªß ƒë·ªÅ ch√≠nh"""
        theme_keywords = {
            't√¨nh y√™u': ['y√™u', 't√¨nh y√™u', 'l√£ng m·∫°n', 'h·∫πn h√≤', 'c∆∞·ªõi'],
            'gia ƒë√¨nh': ['gia ƒë√¨nh', 'b·ªë m·∫π', 'con c√°i', 'anh em', 'h·ªç h√†ng'],
            't√¨nh b·∫°n': ['b·∫°n b√®', 't√¨nh b·∫°n', 'ƒë·ªìng nghi·ªáp', 'c√πng nhau'],
            's·ª± nghi·ªáp': ['c√¥ng vi·ªác', 's·ª± nghi·ªáp', 'th√†nh c√¥ng', 'th·∫•t b·∫°i', 'c·ªë g·∫Øng'],
            'c√¥ng l√Ω': ['c√¥ng l√Ω', 'ƒë√∫ng sai', 'thi·ªán √°c', 'ch√≠nh nghƒ©a'],
            'phi√™u l∆∞u': ['phi√™u l∆∞u', 'kh√°m ph√°', 'm·∫°o hi·ªÉm', 'du l·ªãch'],
            'h√†i h∆∞·ªõc': ['vui nh·ªôn', 'h√†i h∆∞·ªõc', 'c∆∞·ªùi', 'gi·∫£i tr√≠']
        }
        
        text_lower = text.lower()
        theme_scores = {}
        
        for theme, keywords in theme_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                theme_scores[theme] = score
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë
        sorted_themes = sorted(theme_scores.items(), key=lambda x: x[1], reverse=True)
        
        return [theme for theme, score in sorted_themes]
    
    def _save_to_database(self, original_script: str, result: Dict, script_id: Optional[str] = None):
        """L∆∞u k·∫øt qu·∫£ v√†o MongoDB"""
        try:
            # T·∫°o document cho AI summary
            ai_summary_doc = {
                'script_id': ObjectId(script_id) if script_id else None,
                'original_script': original_script,
                'summary_result': result,
                'genre': result['genre'],
                'genre_confidence': result['genre_confidence'],
                'category_id': ObjectId(result['category_id']) if result.get('category_id') else None,
                'total_episodes': result['total_episodes'],
                'quality_score': self._calculate_quality_score(result),
                'created_at': datetime.now(),
                'updated_at': datetime.now(),
                'feedback_count': 0,
                'average_rating': 0.0,
                'ai_version': '1.0.0',
                'processing_stats': {
                    'scenes_extracted': result['script_stats']['total_scenes_extracted'],
                    'original_word_count': result['script_stats']['word_count'],
                    'summary_word_count': sum(ep['word_count'] for ep in result['episodes'])
                }
            }
            
            inserted = self.ai_summaries_collection.insert_one(ai_summary_doc)
            result['ai_summary_id'] = str(inserted.inserted_id)
            print(f"‚úÖ Saved AI summary to database with ID: {inserted.inserted_id}")
            
        except Exception as e:
            print(f"‚ùå Error saving to database: {e}")
    def get_all_summaries(self, limit: int = 100):
        """L·∫•y danh s√°ch c√°c AI summary"""
        try:
            summaries_cursor = self.ai_summaries_collection.find().sort("created_at", -1).limit(limit)
            summaries = []
            for doc in summaries_cursor:
                summaries.append({
                    "id": str(doc["_id"]),
                    "script_id": str(doc["script_id"]) if doc["script_id"] else None,
                    "genre": doc.get("genre"),
                    "genre_confidence": doc.get("genre_confidence"),
                    "total_episodes": doc.get("total_episodes"),
                    "quality_score": doc.get("quality_score"),
                    "created_at": doc.get("created_at"),
                    "average_rating": doc.get("average_rating"),
                    "feedback_count": doc.get("feedback_count")
                })
            return summaries
        except Exception as e:
            print(f"‚ùå Error fetching summaries: {e}")
            raise
    
    def _calculate_quality_score(self, result: Dict) -> float:
        """T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng t√≥m t·∫Øt"""
        score = 0.5  # Base score
        
        # ƒêi·ªÉm d·ª±a tr√™n genre confidence
        score += result['genre_confidence'] * 0.2
        
        # ƒêi·ªÉm d·ª±a tr√™n s·ªë l∆∞·ª£ng episodes
        if 2 <= result['total_episodes'] <= 10:
            score += 0.1
        
        # ƒêi·ªÉm d·ª±a tr√™n ƒë·ªô d√†i t√≥m t·∫Øt
        for episode in result['episodes']:
            word_count = episode['word_count']
            if 550 <= word_count <= 650:  # ƒê√∫ng ƒë·ªô d√†i A4
                score += 0.05
        
        # ƒêi·ªÉm d·ª±a tr√™n s·ªë c·∫£nh ƒë∆∞·ª£c tr√≠ch xu·∫•t
        scenes_count = result['script_stats']['total_scenes_extracted']
        if scenes_count >= 10:
            score += 0.1
        
        return min(score, 1.0)
    
    def get_feedbacks_for_training(self, limit: int = 100) -> List[Dict]:
        """L·∫•y feedback t·ª´ DB ƒë·ªÉ training"""
        try:
            # L·∫•y feedback c√≥ rating v√† content
            feedbacks = list(self.feedbacks_collection.find(
                {
                    "isVisible": True,
                    "isFlagged": False,
                    "content": {"$exists": True, "$ne": ""},
                    "rate": {"$exists": True}
                },
                {
                    "content": 1,
                    "rate": 1,
                    "movieId": 1,
                    "storyId": 1,
                    "createdAt": 1,
                    "helpfulCount": 1
                }
            ).sort("createdAt", -1).limit(limit))
            
            return feedbacks
        except Exception as e:
            print(f"‚ùå Error fetching feedbacks: {e}")
            return []
    
    def train_from_feedback(self, feedback_list: List[str] = None) -> Dict:
        """Hu·∫•n luy·ªán model t·ª´ feedback"""
        try:
            # N·∫øu kh√¥ng c√≥ feedback_list, l·∫•y t·ª´ DB
            if not feedback_list:
                db_feedbacks = self.get_feedbacks_for_training()
                feedback_list = [fb['content'] for fb in db_feedbacks if fb.get('content')]
            
            if not feedback_list:
                return {'error': 'Kh√¥ng c√≥ feedback ƒë·ªÉ hu·∫•n luy·ªán'}
            
            # Ph√¢n t√≠ch feedback v√† c·∫≠p nh·∫≠t model
            training_result = self._process_feedback_for_training(feedback_list)
            
            # L∆∞u training metrics
            training_metrics = {
                'feedback_count': len(feedback_list),
                'positive_feedback': sum(1 for f in feedback_list if self._analyze_feedback_sentiment(f) == 'positive'),
                'negative_feedback': sum(1 for f in feedback_list if self._analyze_feedback_sentiment(f) == 'negative'),
                'training_improvements': training_result,
                'trained_at': datetime.now(),
                'ai_version': '1.0.0'
            }
            
            self.training_metrics_collection.insert_one(training_metrics)
            
            return {
                'success': True,
                'feedback_processed': len(feedback_list),
                'improvements': training_result,
                'training_id': str(training_metrics.get('_id'))
            }
            
        except Exception as e:
            return {'error': f'L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}'}
    
    def _analyze_feedback_sentiment(self, feedback: str) -> str:
        """Ph√¢n t√≠ch sentiment c·ªßa feedback"""
        positive_words = ['t·ªët', 'hay', 'xu·∫•t s·∫Øc', 'tuy·ªát v·ªùi', '·ªïn', 'ƒë√∫ng', 'ch√≠nh x√°c', 'h√†i l√≤ng', 'th√≠ch', 'y√™u']
        negative_words = ['t·ªá', 'sai', 'kh√¥ng t·ªët', 'k√©m', 'thi·∫øu', 'kh√¥ng ƒë√∫ng', 'kh√¥ng hay', 'd·ªü', 'ch√°n']
        
        feedback_lower = feedback.lower()
        positive_count = sum(1 for word in positive_words if word in feedback_lower)
        negative_count = sum(1 for word in negative_words if word in feedback_lower)
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
    
    def _process_feedback_for_training(self, feedback_list: List[str]) -> Dict:
        """X·ª≠ l√Ω feedback ƒë·ªÉ c·∫£i thi·ªán model"""
        improvements = {
            'genre_detection': 0,
            'scene_extraction': 0,
            'summary_quality': 0,
            'episode_structure': 0,
            'character_analysis': 0
        }
        
        for feedback in feedback_list:
            feedback_lower = feedback.lower()
            
            # C·∫£i thi·ªán genre detection
            if any(word in feedback_lower for word in ['th·ªÉ lo·∫°i', 'genre', 'sai th·ªÉ lo·∫°i', 'kh√¥ng ƒë√∫ng th·ªÉ lo·∫°i']):
                improvements['genre_detection'] += 1
            
            # C·∫£i thi·ªán scene extraction
            if any(word in feedback_lower for word in ['c·∫£nh', 'thi·∫øu', 'b·ªè s√≥t', 'kh√¥ng ƒë·ªß', 'c·∫ßn th√™m']):
                improvements['scene_extraction'] += 1
            
            # C·∫£i thi·ªán summary quality
            if any(word in feedback_lower for word in ['t√≥m t·∫Øt', 'ch·∫•t l∆∞·ª£ng', 'chi ti·∫øt', 'ng·∫Øn g·ªçn']):
                improvements['summary_quality'] += 1
            
            # C·∫£i thi·ªán episode structure
            if any(word in feedback_lower for word in ['t·∫≠p', 'c·∫•u tr√∫c', 'chia', 'ph·∫ßn']):
                improvements['episode_structure'] += 1
                
            # C·∫£i thi·ªán character analysis
            if any(word in feedback_lower for word in ['nh√¢n v·∫≠t', 'character', 'vai di·ªÖn', 'di·ªÖn vi√™n']):
                improvements['character_analysis'] += 1
        
        # L∆∞u learned patterns
        self._save_learned_patterns(improvements)
        
        return improvements
    
    def _save_learned_patterns(self, improvements: Dict):
        """L∆∞u patterns ƒë√£ h·ªçc"""
        pattern_doc = {
            'improvements': improvements,
            'learned_at': datetime.now(),
            'pattern_type': 'feedback_analysis',
            'ai_version': '1.0.0'
        }
        self.learned_patterns_collection.insert_one(pattern_doc)
    
    def _load_models(self):
        """Load models ƒë√£ ƒë∆∞·ª£c train tr∆∞·ªõc ƒë√≥"""
        try:
            # T·∫°o sample data ƒë·ªÉ train model ban ƒë·∫ßu
            self._initialize_models_with_sample_data()
            print("‚úÖ Models initialized successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not load pre-trained models: {e}")
    
    def _initialize_models_with_sample_data(self):
        """Kh·ªüi t·∫°o models v·ªõi sample data"""
        # Sample data cho genre classification
        sample_texts = [
            "Anh y√™u em nhi·ªÅu l·∫Øm. Ch√∫ng ta s·∫Ω c∆∞·ªõi nhau.",
            "S√∫ng n·ªï vang. Anh ta lao v√†o cu·ªôc chi·∫øn ƒë·∫•u.",
            "C∆∞·ªùi l·ªõn. T√¨nh hu·ªëng n√†y th·∫≠t bu·ªìn c∆∞·ªùi.",
            "Gia ƒë√¨nh t√¥i r·∫•t h·∫°nh ph√∫c. T√¨nh c·∫£m s√¢u s·∫Øc.",
            "B√≠ ·∫©n ƒë·∫±ng sau c√°i ch·∫øt. S·ª£ h√£i bao tr√πm."
        ]
        
        sample_labels = ['romance', 'action', 'comedy', 'drama', 'thriller']
        
        try:
            # Fit vectorizer v·ªõi sample data
            self.vectorizer.fit(sample_texts)
            
            # Train genre classifier v·ªõi sample data
            X_sample = self.vectorizer.transform(sample_texts)
            self.genre_classifier.fit(X_sample, sample_labels)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error initializing models: {e}")

    def get_training_statistics(self) -> Dict:
        """L·∫•y th·ªëng k√™ training"""
        try:
            # Th·ªëng k√™ t·ª´ AI summaries
            total_summaries = self.ai_summaries_collection.count_documents({})
            
            # Th·ªëng k√™ t·ª´ feedbacks
            total_feedback = self.feedbacks_collection.count_documents({
                "isVisible": True,
                "isFlagged": False
            })
            
            # Feedback g·∫ßn ƒë√¢y (7 ng√†y)
            week_ago = datetime.now() - timedelta(days=7)
            recent_feedback = self.feedbacks_collection.count_documents({
                'createdAt': {'$gte': week_ago},
                "isVisible": True,
                "isFlagged": False
            })
            
            # Average quality score t·ª´ AI summaries
            pipeline = [
                {'$group': {'_id': None, 'avg_quality': {'$avg': '$quality_score'}}}
            ]
            avg_quality_result = list(self.ai_summaries_collection.aggregate(pipeline))
            avg_quality = avg_quality_result[0]['avg_quality'] if avg_quality_result else 0
            
            # Average rating t·ª´ feedbacks
            rating_pipeline = [
                {'$match': {'isVisible': True, 'isFlagged': False}},
                {'$group': {'_id': None, 'avg_rating': {'$avg': '$rate'}}}
            ]
            avg_rating_result = list(self.feedbacks_collection.aggregate(rating_pipeline))
            avg_rating = avg_rating_result[0]['avg_rating'] if avg_rating_result else 0
            
            # Training metrics
            training_count = self.training_metrics_collection.count_documents({})
            
            return {
                'total_summaries_generated': total_summaries,
                'total_feedback_received': total_feedback,
                'recent_feedback_count': recent_feedback,
                'average_quality_score': round(avg_quality, 2),
                'average_user_rating': round(avg_rating, 2),
                'total_training_sessions': training_count,
                'last_updated': datetime.now(),
                'ai_version': '1.0.0'
            }
            
        except Exception as e:
            return {'error': f'L·ªói l·∫•y th·ªëng k√™: {str(e)}'}

    def get_script_by_id(self, script_id: str) -> Optional[Dict]:
        """L·∫•y script t·ª´ DB theo ID"""
        try:
            script = self.scripts_collection.find_one({"_id": ObjectId(script_id)})
            if script:
                script['_id'] = str(script['_id'])
                if script.get('movieId'):
                    script['movieId'] = str(script['movieId'])
            return script
        except Exception as e:
            print(f"‚ùå Error fetching script: {e}")
            return None

    def update_summary_feedback(self, ai_summary_id: str, feedback_rating: float, feedback_text: str = ""):
        """C·∫≠p nh·∫≠t feedback cho AI summary"""
        try:
            # C·∫≠p nh·∫≠t AI summary v·ªõi feedback
            update_data = {
                'feedback_count': {'$inc': 1},
                'last_feedback_at': datetime.now()
            }
            
            # T√≠nh l·∫°i average rating
            existing_summary = self.ai_summaries_collection.find_one({"_id": ObjectId(ai_summary_id)})
            if existing_summary:
                current_avg = existing_summary.get('average_rating', 0.0)
                current_count = existing_summary.get('feedback_count', 0)
                new_avg = ((current_avg * current_count) + feedback_rating) / (current_count + 1)
                update_data['average_rating'] = round(new_avg, 2)
            
            self.ai_summaries_collection.update_one(
                {"_id": ObjectId(ai_summary_id)},
                {"$set": update_data}
            )
            
            print(f"‚úÖ Updated feedback for AI summary: {ai_summary_id}")
            
        except Exception as e:
            print(f"‚ùå Error updating summary feedback: {e}")

